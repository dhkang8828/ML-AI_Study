{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334f694b",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24223b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd8b7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                           [.4, .5, .6],\n",
    "                           [.7, .8, .9]]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5b33b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8943, 0.5098, 0.2808],\n",
       "        [0.0040, 0.7474, 0.7890],\n",
       "        [0.9465, 0.1708, 0.0137]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target).cuda() ## the final scalar will be differentiate by x.\n",
    "x.requires_grad = True      ## Get gradient of x, after differentiation. \n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8edd3f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2470, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th loss:0.14943909645080566\n",
      "tensor([[0.7178, 0.4410, 0.2850],\n",
      "        [0.0920, 0.6924, 0.7470],\n",
      "        [0.8917, 0.3106, 0.2106]], device='cuda:0', requires_grad=True)\n",
      "2-th loss:0.09040144085884094\n",
      "tensor([[0.5805, 0.3874, 0.2884],\n",
      "        [0.1604, 0.6497, 0.7143],\n",
      "        [0.8491, 0.4194, 0.3638]], device='cuda:0', requires_grad=True)\n",
      "3-th loss:0.05468728765845299\n",
      "tensor([[0.4737, 0.3458, 0.2910],\n",
      "        [0.2137, 0.6164, 0.6889],\n",
      "        [0.8160, 0.5040, 0.4830]], device='cuda:0', requires_grad=True)\n",
      "4-th loss:0.03308243304491043\n",
      "tensor([[0.3907, 0.3134, 0.2930],\n",
      "        [0.2551, 0.5905, 0.6692],\n",
      "        [0.7902, 0.5698, 0.5756]], device='cuda:0', requires_grad=True)\n",
      "5-th loss:0.020012829452753067\n",
      "tensor([[0.3261, 0.2882, 0.2945],\n",
      "        [0.2873, 0.5704, 0.6538],\n",
      "        [0.7702, 0.6209, 0.6477]], device='cuda:0', requires_grad=True)\n",
      "6-th loss:0.012106526643037796\n",
      "tensor([[0.2758, 0.2686, 0.2957],\n",
      "        [0.3123, 0.5548, 0.6418],\n",
      "        [0.7546, 0.6607, 0.7038]], device='cuda:0', requires_grad=True)\n",
      "7-th loss:0.007323701400309801\n",
      "tensor([[0.2368, 0.2533, 0.2967],\n",
      "        [0.3318, 0.5426, 0.6325],\n",
      "        [0.7424, 0.6917, 0.7474]], device='cuda:0', requires_grad=True)\n",
      "8-th loss:0.004430388566106558\n",
      "tensor([[0.2064, 0.2415, 0.2974],\n",
      "        [0.3470, 0.5331, 0.6253],\n",
      "        [0.7330, 0.7157, 0.7813]], device='cuda:0', requires_grad=True)\n",
      "9-th loss:0.0026801121421158314\n",
      "tensor([[0.1827, 0.2323, 0.2980],\n",
      "        [0.3588, 0.5258, 0.6197],\n",
      "        [0.7257, 0.7345, 0.8077]], device='cuda:0', requires_grad=True)\n",
      "10-th loss:0.0016213018679991364\n",
      "tensor([[0.1643, 0.2251, 0.2984],\n",
      "        [0.3679, 0.5200, 0.6153],\n",
      "        [0.7200, 0.7490, 0.8282]], device='cuda:0', requires_grad=True)\n",
      "11-th loss:0.000980787561275065\n",
      "tensor([[0.1500, 0.2195, 0.2988],\n",
      "        [0.3750, 0.5156, 0.6119],\n",
      "        [0.7155, 0.7604, 0.8442]], device='cuda:0', requires_grad=True)\n",
      "12-th loss:0.0005933159263804555\n",
      "tensor([[0.1389, 0.2152, 0.2991],\n",
      "        [0.3806, 0.5121, 0.6093],\n",
      "        [0.7121, 0.7692, 0.8566]], device='cuda:0', requires_grad=True)\n",
      "13-th loss:0.00035891993320547044\n",
      "tensor([[0.1303, 0.2118, 0.2993],\n",
      "        [0.3849, 0.5094, 0.6072],\n",
      "        [0.7094, 0.7760, 0.8662]], device='cuda:0', requires_grad=True)\n",
      "14-th loss:0.00021712412126362324\n",
      "tensor([[0.1235, 0.2092, 0.2994],\n",
      "        [0.3883, 0.5073, 0.6056],\n",
      "        [0.7073, 0.7813, 0.8737]], device='cuda:0', requires_grad=True)\n",
      "15-th loss:0.00013134675100445747\n",
      "tensor([[0.1183, 0.2071, 0.2996],\n",
      "        [0.3909, 0.5057, 0.6044],\n",
      "        [0.7057, 0.7855, 0.8796]], device='cuda:0', requires_grad=True)\n",
      "16-th loss:7.945676770759746e-05\n",
      "tensor([[0.1142, 0.2056, 0.2997],\n",
      "        [0.3929, 0.5044, 0.6034],\n",
      "        [0.7044, 0.7887, 0.8841]], device='cuda:0', requires_grad=True)\n",
      "17-th loss:4.806639117305167e-05\n",
      "tensor([[0.1111, 0.2043, 0.2997],\n",
      "        [0.3945, 0.5035, 0.6026],\n",
      "        [0.7034, 0.7912, 0.8876]], device='cuda:0', requires_grad=True)\n",
      "18-th loss:2.9077238650643267e-05\n",
      "tensor([[0.1086, 0.2034, 0.2998],\n",
      "        [0.3957, 0.5027, 0.6021],\n",
      "        [0.7027, 0.7932, 0.8904]], device='cuda:0', requires_grad=True)\n",
      "19-th loss:1.758994949341286e-05\n",
      "tensor([[0.1067, 0.2026, 0.2998],\n",
      "        [0.3967, 0.5021, 0.6016],\n",
      "        [0.7021, 0.7947, 0.8925]], device='cuda:0', requires_grad=True)\n",
      "20-th loss:1.064083608071087e-05\n",
      "tensor([[0.1052, 0.2020, 0.2999],\n",
      "        [0.3974, 0.5016, 0.6012],\n",
      "        [0.7016, 0.7959, 0.8942]], device='cuda:0', requires_grad=True)\n",
      "21-th loss:6.4370451582362875e-06\n",
      "tensor([[0.1041, 0.2016, 0.2999],\n",
      "        [0.3980, 0.5013, 0.6010],\n",
      "        [0.7013, 0.7968, 0.8955]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1\n",
    "iter_cnt = 0 \n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1 \n",
    "    loss.backward()         ##Calculate Gradient. \n",
    "                            ##loss라는 스칼라값에 (편)미분 실시\n",
    "                            ## x.grad에 값이 들어있다 x.grad = (3, 3)\n",
    "    \n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    ## ignore below 2 line right-now\n",
    "    x.detach_()     ## 지금 단계에선 끊어준다고만 이해하고 넘어가자\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print(f\"{iter_cnt}-th loss:{loss}\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230bee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
